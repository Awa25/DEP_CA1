---
title: "Data Exploration & Preparation -  CA1 Project"
author: "Chia Hua Lin-2020044 & Ruei Li Jhang-2020443"
output: html_document
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



## About the Dataset

The data is about Coronavirus (COVID-19) Vaccinations

The website states that the vaccine dataset is based on the most current official data from international health ministries and governments. The United Nations World Population Prospects serve as the basis for population estimates used in per-capita measurements. Based on the World Bank classification, income groups are created. Source: https://ourworldindata.org/covid-vaccinations



```{r message=FALSE}
# Load required libraries
library(tidyverse)
library(lares) # show the structure of data
library(vtable) # show the mean, sd, min, max
library(caret) 
library(psycho) # Standardizing
library('DescTools')
library(ggcorrplot)
library(plotly) # Using plotly for interactive plots
library(fastDummies) # for dummy encoding
```


## Loading the dataset

```{r}

# Loading the data after downloading
data <- read.csv("owid-covid-data.csv")
dim(data)

# Overview of the data
head(data, 5)

str(data)

```



### Checking for missing values

```{r, message=FALSE, warning=FALSE}

# Check for null values in the dataframe
is_null <- is.na(data)

# Count the number of null values in each column
num_null_values <- colSums(is_null, na.rm = TRUE)

# Print the number of null values in each column
print(num_null_values)

# show the details of the dataset
df_str(data, return = "plot")
```




As we can see from the above output, there are columns with a very high number of null values. So the first thing would be to drop all the columns whose number of null values is more more than half the total number of rows, or observations.

**Sometimes, R do not read empty strings, and question marks as nulls. So we first convert the Question marks and the empty strings as nulls then check the number of null values.



```{r}

# Replace question marks with NA in the entire dataframe
data[data == "?"] <- NA

# Replace empty strings with NA in the entire dataframe
data[data == ""] <- NA

# Check for null values in each column
null_counts <- colSums(is.na(data))

# Get columns where more than half of the values are null
columns_to_drop <- names(null_counts[null_counts > nrow(data)/2])

# Drop columns with more than half of the values being null
data <- data[, !names(data) %in% columns_to_drop]

dim(data)

# Drop rows with NA values using na.omit()
data <- na.omit(data)

# Check for null values in the dataframe
is_null <- is.na(data)

# Count the number of null values in each column
num_null_values <- colSums(is_null, na.rm = TRUE)

# Print the number of null values in each column
print(num_null_values)

View(data)
```



We can now see that we do not have any null values in the columns.

### Dropping all the Null Values

```{r}

# Drop rows with NA values using na.omit()
data_clean <- na.omit(data)

# checking the shape of the data
cat('The shape of the data:', dim(data_clean)[1], 'rows/observations', 'and', dim(data_clean)[2], 'columns')

```

The Dataset is already eligible for our analysis as it means the eligibility of 7000 and 10 columns



### Removing Unwanted Columns

```{r}

# Removing a single column using subset() function
data <- subset(data, select = -c(population, human_development_index, life_expectancy, hospital_beds_per_thousand, male_smokers, female_smokers,
diabetes_prevalence, cardiovasc_death_rate, gdp_per_capita, aged_70_older, aged_65_older, median_age, population_density, stringency_index, new_people_vaccinated_smoothed_per_hundred, new_vaccinations_smoothed_per_million, new_deaths_smoothed_per_million, new_deaths_per_million, total_deaths_per_million, new_cases_smoothed_per_million, new_cases_per_million, total_cases_per_million))

dim(data)
```














